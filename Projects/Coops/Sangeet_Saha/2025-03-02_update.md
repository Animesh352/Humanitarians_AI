# Biweekly Report 3
**Name:** Sangeet Saha  
**Date Range:** February 17 â€“ March 2  
**Team/Project Name:** Chain of Thought Multi-Agent Reasoning (Humanitarian AI)

## Hours
Total hours this period: 80

## Highlights
- Developed robust API endpoints for Groq, allowing modular use of hosted and local models.
- Refactored codebase to support plug-and-play behavior between Groq and Ollama.
- Finalized agent creation logic with step-by-step chain-of-thought merging into structured JSON format.
- Conducted extensive system testing for model outputs, agent behaviors, and data consistency.
- Developed system prompts tailored for specific reasoning contexts: summarization, planning, conflict resolution, etc.
- Authored comprehensive documentation outlining system architecture, endpoint usage, agent behavior, and prompt design.

## In Progress
- Working on config-driven behavior modification using LLMConfig and AgentConfig classes.
- Fine-tuning system prompt templates for specific task categories.
- Internal demos to get feedback on usability and clarity of outputs.

## Next Steps
[Section intentionally left blank as per original document]

## Team Interactions
### Wednesday Standups (9:00 AM):
- **Discussion Summary:** Shared progress on agent merging and JSON structure.
- **Feedback:** Very positive; team appreciated clean output and modularity.
- **Demo Shared:** JSON response demo with agent breakdown.

### Friday Standups (9:00 AM):
- **Discussion Summary:** Finalizing system prompt templates; discussed evaluation metrics.
- **Feedback:** Encouraged to formalize configuration files for reuse.
- **Demo Shared:** Prompt template switching in real time.

### Other Meetings/Collaborations:
- Feb 20: Review meeting for agent behavior consistency and failure recovery.
- Feb 28: Final project walkthrough with architecture and config-driven controls.

## Learnings & Reflections
- Learned the value of agent modularity for scalable, reusable AI workflows.
- Understood how prompt engineering can drastically affect LLM performance across reasoning types.
- Developed deeper expertise in chaining multiple LLMs in a cooperative reasoning architecture.

## Blockers
- Slight learning curve in balancing verbosity and precision in system prompts.
- Occasional throttling limits with hosted LLMs while testing at scale.